# Natural Language Processing
- ğŸ¤—[papers-with- code](https://paperswithcode.com/area/natural-language-processing)1769 benchmarks â€¢ 541 tasks â€¢ 1515 datasets â€¢ 15957 papers with code


## [Language Modelling](https://paperswithcode.com/task/language-modelling)
- 1989 papers with code â€¢ 37 benchmarks â€¢ 130 datasets
- ğŸ¤—[huggingface/transformers](https://github.com/huggingface/transformers)

- NLPæœ€æ–°ç™¼å±•:Large-scale pretrained language models
- Amazon AlexaTM 20B(202208)
  - [æ–°è:Amazonç™¾å„„åƒæ•¸æ¨¡å‹å°æ¨£æœ¬å­¸ç¿’å‹éGPT-3ã€PaLMåƒå„„åƒæ•¸å¤§æ¨¡å‹(2022-08-04)](https://www.ithome.com.tw/news/152283)
  - [è«–æ–‡:AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model(202208)](https://arxiv.org/abs/2208.01448)
  - [20B-parameter Alexa model sets new marks in few-shot learning(20220802)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)
- Google PaLM(202204)
  - [è«–æ–‡:PaLM: Scaling Language Modeling with Pathways(Google Research)(202204)](https://arxiv.org/abs/2204.02311)
  - [å¯¦ä½œ:PaLM - Pytorch](https://github.com/lucidrains/PaLM-pytorch)
- OpenAI GPT-3(2020)
  - [Language Models are Few-Shot Learners(202005)](https://arxiv.org/pdf/2005.14165.pdf)
  - [å¾®è»Ÿæ–¼Microsoft Power Appsä¸­åµŒå…¥GPT-3ï¼Œä»¥è‡ªç„¶èªè¨€å°±èƒ½é–‹ç™¼æ‡‰ç”¨(2021-05-27)](https://www.ithome.com.tw/news/144649)
  - ğŸ‘ğŸ»ğŸ‘ğŸ»[gpt3-and-cybersecurity/spam_detector](https://github.com/sophos/gpt3-and-cybersecurity/tree/main/spam_detector) 
  - [è«–æ–‡:Improving Short Text Classification With Augmented Data Using GPT-3(202205)](https://arxiv.org/abs/2205.10981)
  - [è«–æ–‡:Language Models are Few-Shot Learners(202005)](https://arxiv.org/abs/2005.14165)
- Google Text-To-Text Transfer Transformer(T5)
  - [è«–æ–‡:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(201910)](https://arxiv.org/abs/1910.10683)
  - [T5X:GITHUB](https://github.com/google-research/text-to-text-transfer-transformer)
- HyperCLOVA(2021)
  - [AIè¶¨å‹¢å‘¨å ±ç¬¬170æœŸï¼šNaverç™¼å¸ƒGPT-3ç­‰ç´šçš„è¶…å¤§éŸ“æ–‡æ¨¡å‹HyperCLOVAï¼Œæ‡‚èªè¨€é‚„è¦æ‡‚åœ–ç‰‡å’Œå½±ç‰‡(2021-06-05)]()
  - HyperCLOVAæœ‰2,040å„„åƒæ•¸ï¼Œæ¯”å…¬èªæ˜¯NLPä¸–ç•Œæ¨™æº–çš„GPT-3ï¼Œé‚„è¦å¤šå‡º1,750å„„å€‹åƒæ•¸ã€‚åƒæ•¸è¶Šå¤šï¼Œå°±è¶Šèƒ½ç´°ç·»åœ°è¾¨è­˜èªè¨€ã€‚ 
- XLNet(2019)
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding(201906)](https://arxiv.org/abs/1906.08237)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach(201907)](https://arxiv.org/abs/1907.11692)
- [Cross-lingual Language Model Pretraining(201901)](https://arxiv.org/abs/1901.07291)
- Transformer-XL(2019)
  - [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context(201901)](https://arxiv.org/abs/1901.02860)
- BERT(2018)
  - [è«–æ–‡:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(201810)](https://arxiv.org/abs/1810.04805)
  - [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) 
  - YOUTUEå½±ç‰‡
    - [BERT Neural Network - EXPLAINED!](https://www.youtube.com/watch?v=xI0HHN5XKDo)
  - Pytorchå¯¦ä½œ
    - [PYTORCH-TRANSFORMERS By HuggingFace Teamsä½ è¦çš„å…¨åœ¨æ­¤](https://pytorch.org/hub/huggingface_pytorch-transformers/)
    - []()
      - pip install bert-pytorch 
- ULM-Fit(201801)Universal Language Model Fine-tuning 
  - [Universal Language Model Fine-tuning for Text Classification(201801)](https://arxiv.org/abs/1801.06146) 
  - Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. 
  -  propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. 
  -  Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. 
  -  Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. 
  -  We open-source our pretrained models and code.
- ELMo(201802)
  - [Deep contextualized word representations(201802)](https://arxiv.org/pdf/1802.05365.pdf)  
- GPT-2(2019):
  - [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) 
  - [Pytorchç‰ˆæœ¬](https://github.com/huggingface/transformers)
  - [GPT-2ä¸­æ–‡ç‰ˆæœ¬](https://github.com/Morizeyao/GPT2-Chinese)
    - [NLPæ¨¡å‹æ‡‰ç”¨ä¹‹ä¸‰ï¼šGPTèˆ‡GPT-2](https://www.twblogs.net/a/5efb005bfa148015395f47a8) 
    - git clone https://github.com/Morizeyao/GPT2-Chinese 
- OpenAI GPT:Generative Pre-Training(201808)
  - [Improving Language Understandingby Generative Pre-Training(2018)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) 

## [Text Classification](https://paperswithcode.com/task/text-classification)
- 743 papers with code â€¢ 107 benchmarks â€¢ 102 datasets
- [REVIEW: Text Classification Algorithms: A Survey(201904)](https://arxiv.org/pdf/1904.08067v4.pdf)
## Document Classification
- [DocBERT: BERT for Document Classification(201904)](https://arxiv.org/abs/1904.08398)
  - ğŸ‘ğŸ»ğŸ‘[PyTorchå¯¦ä½œ](https://github.com/castorini/hedwig) 
- [Document Classification using BERT](https://www.kaggle.com/code/merishnasuwal/document-classification-using-bert)
