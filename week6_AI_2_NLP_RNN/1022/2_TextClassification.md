# Text Classification
- [Text Classification: 743 papers with code â€¢ 109 benchmarks â€¢ 102 datasets](https://paperswithcode.com/task/text-classification)
- [Text Classification on IMDb@State Of the Art](https://paperswithcode.com/sota/text-classification-on-imdb)

# ç¯„ä¾‹è³‡æ–™é›†IMDB dataset
- ç¶²è·¯é›»å½±è³‡æ–™åº«ï¼ˆInternet Movie Databaseï¼‰çš„ IMDB è³‡æ–™é›†ï¼ˆIMDB datasetï¼‰
- åŒ…å« 50,000 æ¢å½±è©•æ–‡æœ¬ã€‚
- å¾è©²è³‡æ–™é›†åˆ‡å‰²å‡ºçš„25,000æ¢è©•è«–ç”¨ä½œè¨“ç·´ï¼Œå¦å¤– 25,000 æ¢ç”¨ä½œæ¸¬è©¦ã€‚
- è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†æ˜¯å¹³è¡¡çš„ï¼ˆbalancedï¼‰ï¼Œæ„å‘³è‘—å®ƒå€‘åŒ…å«ç›¸ç­‰æ•¸é‡çš„ç©æ¥µå’Œæ¶ˆæ¥µè©•è«–ã€‚
- IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. 
- These are split into 25,000 reviews for training and 25,000 reviews for testing. 
- The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.
- ğŸ“½ï¸ğŸ¬ [Sentiment Analysis of IMDB Movie Reviews | Kaggle(503)](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
  - [IMDB review Word2Vec & BiLSTM - 99% acc](https://www.kaggle.com/code/alexcherniuk/imdb-review-word2vec-bilstm-99-acc) 
  - [IMDB Review - Deep Model ~ 94.89% Accuracy](https://www.kaggle.com/code/nilanml/imdb-review-deep-model-94-89-accuracy/notebook)
  - [IMDB Movie Reviews word2vec,tfidf,bow](https://www.kaggle.com/code/jagarapusiva/imdb-movie-reviews-word2vec-tfidf-bow)

# ç¯„ä¾‹ç¨‹å¼  é›»å½±è©•è«–æ–‡æœ¬åˆ†é¡
- [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification)
  - text classification starting from plain text files stored on disk. 
  - will train a binary classifier to perform sentiment analysis on an `IMDB dataset`. 
  - tf.keras.utils.text_dataset_from_directory 
- [Text classification with TensorFlow Hub: Movie reviews](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub)
  - use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2 
```
embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])
```
- [Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)
  - ç¶“å…¸çš„Bi_LSTM model 
  - ![model_Bi_LSTM.png](./model_Bi_LSTM.png)
- [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) 
  - ä½¿ç”¨Tensorflow-text
  - pre-train model 
```
def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)
```
- REVIEW[Pre-trained Models for Natural Language Processing: A Survey(2020)](https://arxiv.org/abs/2003.08271)
# ç¯„ä¾‹å­¸ç¿’:æ•™ç§‘æ›¸[Keras å¤§ç¥æ­¸ä½](https://www.tenlong.com.tw/products/9789863127017?list_name=srh)
```
æ•™ç§‘æ›¸[Keras å¤§ç¥æ­¸ä½]
æ·±åº¦å­¸ç¿’å…¨é¢é€²åŒ–ï¼ç”¨ Python å¯¦ä½œCNNã€RNNã€GRUã€LSTMã€GANã€VAEã€Transformer
FranÃ§ois Chollet è‘— é»ƒé€¸è¯ã€æ—é‡‡è–‡ è­¯ é»ƒé€¸è¯ å¯©ã€æ–½å¨éŠ˜ç ”ç©¶å®¤ ç›£ä¿®
```
- ç¯„ä¾‹ç¨‹å¼: [åˆ°å®˜æ–¹ç¶²å€ä¸‹è¼‰](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff) [GITHUB](https://github.com/fchollet/deep-learning-with-python-notebooks) 
- PART 4: Tensorflow RNN
  - ç¬¬10ç« ï¼šæ™‚é–“åºåˆ—çš„æ·±åº¦å­¸ç¿’  [ç¯„ä¾‹ç¨‹å¼](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter10_dl-for-timeseries.ipynb)
  - ğŸ‘ğŸ»ç¬¬11ç« ï¼šæ–‡å­—è³‡æ–™çš„æ·±åº¦å­¸ç¿’
    - 11-1 æ¦‚è¿°è‡ªç„¶èªè¨€è™•ç†(natural language processing, NLP)
    - 11-2 æº–å‚™æ–‡å­—è³‡æ–™
    - 11-3 è¡¨ç¤ºå–®å­—çµ„çš„å…©ç¨®æ–¹æ³•ï¼šé›†åˆ(set)åŠåºåˆ—(sequence)
    - 11-4 [Transformeræ¶æ§‹](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb)
    - 11-5 [æ–‡å­—åˆ†é¡ä¹‹å¤–çš„ä»»å‹™-ä»¥Seq2seqæ¨¡å‹ç‚ºä¾‹](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb)



